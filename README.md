# Multi-Modal Retrieval-Augmented Generation for SEC 10-K Filings

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Paper](https://img.shields.io/badge/paper-PDF-red.svg)](link-to-paper)

> **Academic Project** | CS 582 – Information Retrieval | University of Illinois Chicago | Fall 2025

**Authors:**  
- Anand Meena

---

## Project Overview

This project implements a **Multi-Modal Retrieval-Augmented Generation (RAG)** system that analyzes SEC 10-K financial filings by combining text, tables, and figure captions. The system retrieves relevant information hierarchically and generates accurate, cited answers with mathematical verification.

### Key Achievements

- **+12 point EM improvement** over text-only RAG baselines
- **85% Table Recall@5** for locating relevant tables in long documents
- **96% faithfulness** in generated answers
- **4.2s median latency** for end-to-end query processing

### Key Features

- **Multi-Modal Processing**: Handles text, structured tables, and figure captions as first-class citizens
- **Hierarchical Retrieval**: Two-stage search (sections → content) for improved accuracy
- **Hybrid Search**: Combines dense embeddings with BM25 keyword matching
- **Table-Aware Routing**: Automatically detects and prioritizes table-centric queries
- **Math Verification**: Validates numeric calculations against source data
- **Precise Citations**: Provides section-level and table cell-level references

---

## Data Download (Required)

**Due to the large size of the `data/` folder, it has been zipped and uploaded separately.**

**Download the data folder:** [data.zip on Google Drive](https://drive.google.com/file/d/15hjlGdGdB67XPPdc7AhSyvvlMpV6poKq/view?usp=sharing)

**Setup Instructions:**
1. Download `data.zip` from the Google Drive link above
2. Extract the zip file
3. Place the extracted `data/` folder in the project root directory
4. The structure should be: `project-root/data/`

The data folder includes:
- Downloaded SEC 10-K filings (`data/raw/`)
- Parsed documents (`data/parsed/`)
- Evaluation datasets (`data/evaluation/`)
- Generated indices and results

> **Note:** Without the data folder, you can still review the code and notebooks, but you'll need to run the data ingestion process (notebook 01) to generate your own data.

---

## Quick Start (5 Minutes)

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/sec-10k-multimodal-rag.git
cd sec-10k-multimodal-rag
```

### 2. Install Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 3. Download Data Folder

Download and extract `data.zip` from the Google Drive link above.

### 4. Launch Jupyter

```bash
jupyter notebook
```

### 5. Run Notebooks Sequentially

Navigate to `notebooks/` and open:

1. **01_data_ingestion.ipynb** ← Start here
2. **02_document_parsing.ipynb**
3. **03_indexing.ipynb**
4. **04_retrieval.ipynb**
5. **05_qa_generation.ipynb**
6. **06_evaluation.ipynb**

### 6. Configure Your Settings

Before running `01_data_ingestion.ipynb`, update:

```python
# In the notebook, update this:
USER_AGENT = "YourName your.email@example.com"  # Required by SEC
```

For `05_qa_generation.ipynb`, ensure Ollama is running:

```bash
# Install Ollama from https://ollama.ai
ollama serve

# Pull the model
ollama pull llama3.2:1b
```

---

## Project Structure

```
sec-10k-multimodal-rag/
├── data/                    # Data directory (download separately)
│   ├── raw/                 # Downloaded 10-K filings
│   ├── parsed/              # Parsed documents
│   └── evaluation/          # Benchmark datasets
├── notebooks/               # Jupyter notebooks (main workflow)
│   ├── 01_data_ingestion.ipynb
│   ├── 02_document_parsing.ipynb
│   ├── 03_indexing.ipynb
│   ├── 04_retrieval.ipynb
│   ├── 05_qa_generation.ipynb
│   └── 06_evaluation.ipynb
├── src/                     # Python source code
│   ├── parsers/             # Document and table parsers
│   ├── retrieval/           # Retrieval components
│   ├── qa/                  # QA and verification
│   ├── evaluation/          # Metrics and evaluation
│   └── utils/               # Utilities and config
├── indices/                 # FAISS indices (generated by notebook 03)
├── images/                  # Project visualizations (already in repo)
│   ├── 1_converted.png      # Latency breakdown by component
│   ├── 2_converted.png      # Error distribution by category
│   ├── 3_converted.png      # Ablation study: component impact
│   └── 4_converted.png      # QA metrics & retrieval comparison
├── .gitignore              # Git ignore configuration
├── requirements.txt         # Python dependencies
└── README.md               # This file
```

---

## System Architecture

### Core Components

1. **Document Parser** (`src/parsers/filing_parser.py`)
   - Extracts text, tables, and metadata from HTML/PDF filings
   - Preserves document structure

2. **Table Parser** (`src/parsers/table_parser.py`)
   - Preserves table structure with row/column headers
   - Creates searchable table row-sentences

3. **Section Extractor** (`src/parsers/section_extractor.py`)
   - Identifies key 10-K sections (MD&A, Financial Statements, etc.)
   - Extracts section boundaries

4. **Query Router** (`src/retrieval/query_router.py`)
   - Classifies queries (numeric vs narrative)
   - Detects table-centric queries
   - Routes to appropriate retrieval strategy

5. **Hierarchical Retriever** (`src/retrieval/hierarchical_retriever.py`)
   - **Stage A**: Section-level retrieval (find relevant sections)
   - **Stage B**: Content retrieval within sections (text + tables)
   - Hybrid search (dense + BM25 fusion)

6. **Answer Generator** (`src/qa/answer_generator.py`)
   - LLM-based answer generation
   - Prompt engineering for financial context
   - Citation building

7. **Math Verifier** (`src/qa/math_verifier.py`)
   - Validates numeric calculations against evidence
   - Detects unit mismatches

8. **Evaluation Metrics** (`src/evaluation/metrics.py`)
   - Computes EM, F1, Recall@k, MRR
   - Benchmark comparisons

### Pipeline Flow

```
Query → Route → Stage A (Section Retrieval) → Stage B (Content Retrieval) →
Answer Generation → Math Verification → Citation Building → Final Answer
```

---

## Example Usage

The system handles various question types:

**Numeric Questions:**
```python
"Report the YoY change in R&D expense for 2022 to 2024"
"What is the ratio of long-term debt to equity in 2023?"
```

**Multi-Year Analysis:**
```python
"Which operating segment contributed most to 2024 revenue growth, and by how much?"
```

**Narrative Questions:**
```python
"What are the main risk factors mentioned in the filing?"
"Explain the company's business strategy"
```

---

## Performance Results

| Metric | Target | Achieved |
|--------|--------|----------|
| EM improvement over text-only RAG | +8-12 | ✅ +12 |
| Table Recall@5 | ≥ 0.80 | ✅ 0.85 |
| Faithfulness | ≥ 0.95 | ✅ 0.96 |
| Median Latency | < 8s | ✅ 4.2s |

### System Comparison

| System | EM | F1 | Recall@5 | Latency |
|--------|----|----|----------|---------|
| Text-only RAG | 42.3 | 58.7 | 0.68 | 3.8s |
| Table-only QA | 38.5 | 52.4 | 0.76 | 2.9s |
| Flat Search | 45.1 | 61.2 | 0.72 | 5.6s |
| W/o Math Ver. | 51.8 | 68.5 | 0.83 | 4.0s |
| **Our System** | **54.3** | **71.2** | **0.85** | **4.2s** |

For detailed results and analysis, see the [full project report](docs/project_report.pdf).

### Visualizations

**Ablation Study: Component Impact**

![Ablation Study](images/3_converted.png)

**System Comparison: QA Metrics vs Retrieval/Faithfulness**

![System Comparison](images/4_converted.png)

**Error Distribution by Category**

![Error Distribution](images/2_converted.png)

**Latency Breakdown by Component**

![Latency Breakdown](images/1_converted.png)

---

## Technologies Used

### Document Processing
- **BeautifulSoup** - HTML parsing
- **PyMuPDF** - PDF parsing (planned)

### Embeddings & Search
- **Sentence Transformers** - Dense embeddings (all-mpnet-base-v2)
- **FAISS** - Vector similarity search
- **BM25** - Keyword search

### LLM & Answer Generation
- **Ollama** - Local LLM inference
- **Llama 3.2** - Answer generation

### Data Processing
- **Pandas** - Data manipulation
- **NumPy** - Numerical operations

### Visualization
- **Matplotlib** - Plotting
- **Seaborn** - Statistical visualization

---

## Configuration

### Model Selection

Edit `src/utils/config.py`:

```python
# Embedding model
DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"

# LLM for answer generation
DEFAULT_LLM_MODEL = "llama3.2:1b"  # Via Ollama
```

### Retrieval Parameters

Adjust in `src/utils/config.py`:

```python
DEFAULT_CHUNK_SIZE = 512
DEFAULT_CHUNK_OVERLAP = 50
DEFAULT_TOP_K_SECTIONS = 5
DEFAULT_TOP_K_CONTENT = 10
```

---

## Reproducing Results

### Step-by-Step Guide

#### Step 1: Environment Setup (5 minutes)

```bash
# Install dependencies
pip install -r requirements.txt

# Install Ollama for LLM inference
# Download from https://ollama.ai
ollama serve

# Pull the LLM model
ollama pull llama3.2:1b
```

**Verify:** Check that `ollama list` shows llama3.2:1b

#### Step 2: Data Setup (2 minutes)

```bash
# Download data.zip from Google Drive (see link above)
# Extract to project root
# Verify structure: project-root/data/
```

**Verify:** Run `ls data/raw/*/` and check for HTML files

#### Step 3: Run Notebooks Sequentially (~60-90 minutes total)

Open Jupyter and run notebooks 01-06 in order. Each notebook is self-contained with clear instructions.

#### Step 4: Review Results

After completing all notebooks, check:
- `data/qa_results/qa_results.json` - QA results with citations
- `data/evaluation_report.txt` - Performance metrics
- Console output in notebook 06 - Detailed evaluation

---

## Troubleshooting

### Common Issues

**Issue:** Ollama connection error in notebook 05
```bash
# Solution: Start Ollama server
ollama serve
```

**Issue:** Out of memory during indexing
```python
# Solution: Reduce batch size in notebook 03
embedding_generator = EmbeddingGenerator(
    model_name=embedding_model_name,
    batch_size=16,  # Reduce from 32
    device='cpu'
)
```

**Issue:** FAISS dimension mismatch
```bash
# Solution: Delete indices/ and rerun notebook 03
rm -rf indices/*
```

**Issue:** Module not found
```bash
# Add src to Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
```

---

## Related Work

- [FinQA Dataset](https://github.com/czyssrs/FinQA) - Financial QA benchmark
- [ColBERT](https://github.com/stanford-futuredata/ColBERT) - Efficient multi-stage retrieval
- [TAPAS](https://github.com/google-research/tapas) - Table pre-training
- [RAG Paper](https://arxiv.org/abs/2005.11401) - Original RAG architecture
