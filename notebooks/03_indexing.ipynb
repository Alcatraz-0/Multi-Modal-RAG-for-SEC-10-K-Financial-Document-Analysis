{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Indexing: Build Multi-Modal Search Indices\n",
    "\n",
    "This notebook creates searchable indices for text chunks, table row-sentences, and implements hierarchical section-based retrieval.\n",
    "\n",
    "**Objectives:**\n",
    "- Create dense embeddings for text and table content\n",
    "- Build BM25 keyword index for exact matching\n",
    "- Implement hierarchical indexing (sections → content)\n",
    "- Store metadata for filtering (company, year, section)\n",
    "- Use FAISS for efficient vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Fix OpenMP conflict\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from utils.config import PARSED_DATA_DIR, INDICES_DIR, MODEL_DIR\n",
    "from retrieval.text_chunker import TextChunker\n",
    "from retrieval.embedding_generator import EmbeddingGenerator\n",
    "from retrieval.index_builder import IndexBuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Parsed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parsed documents: 1481\n",
      "Loaded 1481 documents\n"
     ]
    }
   ],
   "source": [
    "# Load parsing log\n",
    "parsing_log = pd.read_csv(PARSED_DATA_DIR / 'parsing_log.csv')\n",
    "successful_parses = parsing_log[parsing_log['status'] == 'success']\n",
    "\n",
    "print(f\"Total parsed documents: {len(successful_parses)}\")\n",
    "\n",
    "# Load all parsed documents\n",
    "all_documents = []\n",
    "for idx, row in successful_parses.iterrows():\n",
    "    with open(row['parsed_file'], 'rb') as f:\n",
    "        doc = pickle.load(f)\n",
    "        all_documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 1481/1481 [00:00<00:00, 3365.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text chunks: 17706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize text chunker\n",
    "chunker = TextChunker(\n",
    "    chunk_size=512,  # tokens\n",
    "    chunk_overlap=50,  # tokens\n",
    "    respect_sentence_boundaries=True\n",
    ")\n",
    "\n",
    "# Prepare chunks from all documents\n",
    "text_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for doc in tqdm(all_documents, desc=\"Chunking text\"):\n",
    "    ticker = doc['metadata']['ticker']\n",
    "    fiscal_year = doc['metadata']['fiscal_year']\n",
    "    \n",
    "    # Chunk each section\n",
    "    for section in doc['sections']:\n",
    "        chunks = chunker.chunk_text(\n",
    "            text=section['content'],\n",
    "            metadata={\n",
    "                'ticker': ticker,\n",
    "                'fiscal_year': fiscal_year,\n",
    "                'section_title': section['title'],\n",
    "                'section_id': section.get('section_id'),\n",
    "                'content_type': 'text'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            text_chunks.append(chunk['text'])\n",
    "            chunk_metadata.append(chunk['metadata'])\n",
    "\n",
    "print(f\"Total text chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Table Row-Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing table sentences: 100%|██████████| 1481/1481 [00:00<00:00, 123029.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total table sentences: 15838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect all table row-sentences\n",
    "table_sentences = []\n",
    "table_metadata = []\n",
    "\n",
    "for doc in tqdm(all_documents, desc=\"Processing table sentences\"):\n",
    "    ticker = doc['metadata']['ticker']\n",
    "    fiscal_year = doc['metadata']['fiscal_year']\n",
    "    \n",
    "    for sent_obj in doc['table_sentences']:\n",
    "        table_sentences.append(sent_obj['sentence'])\n",
    "        table_metadata.append({\n",
    "            'ticker': ticker,\n",
    "            'fiscal_year': fiscal_year,\n",
    "            'table_id': sent_obj['table_id'],\n",
    "            'section': sent_obj.get('section'),\n",
    "            'row_idx': sent_obj['row_idx'],\n",
    "            'content_type': 'table'\n",
    "        })\n",
    "\n",
    "print(f\"Total table sentences: {len(table_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Section Abstracts for Hierarchical Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating section abstracts: 100%|██████████| 1481/1481 [00:00<00:00, 5046.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section abstracts: 14810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create section abstracts (title + first paragraph)\n",
    "section_abstracts = []\n",
    "section_abstract_metadata = []\n",
    "\n",
    "for doc in tqdm(all_documents, desc=\"Creating section abstracts\"):\n",
    "    ticker = doc['metadata']['ticker']\n",
    "    fiscal_year = doc['metadata']['fiscal_year']\n",
    "    \n",
    "    for section in doc['sections']:\n",
    "        # Create abstract: title + first 200 words\n",
    "        content_words = section['content'].split()[:200]\n",
    "        abstract = section['title'] + \". \" + \" \".join(content_words)\n",
    "        \n",
    "        section_abstracts.append(abstract)\n",
    "        section_abstract_metadata.append({\n",
    "            'ticker': ticker,\n",
    "            'fiscal_year': fiscal_year,\n",
    "            'section_title': section['title'],\n",
    "            'section_id': section.get('section_id'),\n",
    "            'content_type': 'section_abstract'\n",
    "        })\n",
    "\n",
    "print(f\"Total section abstracts: {len(section_abstracts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debugging Parsed Documents ===\n",
      "\n",
      "Sample document structure:\n",
      "Keys: dict_keys(['metadata', 'sections', 'tables', 'table_sentences', 'figures'])\n",
      "\n",
      "Metadata: {'ticker': 'MMM', 'fiscal_year': 2024, 'file_format': 'html', 'file_path': 'c:\\\\Users\\\\anand\\\\Desktop\\\\SEM 3\\\\CS 582\\\\Proj\\\\data\\\\raw\\\\MMM\\\\MMM_2024_10K.html', 'num_pages': None, 'num_sections': 10, 'num_tables': 10}\n",
      "Number of sections: 10\n",
      "Number of tables: 10\n",
      "Number of table_sentences: 0\n",
      "\n",
      "First section keys: dict_keys(['title', 'content'])\n",
      "First section title: Section 1\n",
      "First section content length: 5000\n",
      "\n",
      "After processing:\n",
      "Total text chunks: 17706\n",
      "Total section abstracts: 14810\n",
      "Total table sentences: 15838\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check what's in the parsed documents\n",
    "print(\"=== Debugging Parsed Documents ===\")\n",
    "if len(all_documents) > 0:\n",
    "    sample_doc = all_documents[0]\n",
    "    print(f\"\\nSample document structure:\")\n",
    "    print(f\"Keys: {sample_doc.keys()}\")\n",
    "    print(f\"\\nMetadata: {sample_doc.get('metadata', {})}\")\n",
    "    print(f\"Number of sections: {len(sample_doc.get('sections', []))}\")\n",
    "    print(f\"Number of tables: {len(sample_doc.get('tables', []))}\")\n",
    "    print(f\"Number of table_sentences: {len(sample_doc.get('table_sentences', []))}\")\n",
    "    \n",
    "    if sample_doc.get('sections'):\n",
    "        print(f\"\\nFirst section keys: {sample_doc['sections'][0].keys()}\")\n",
    "        print(f\"First section title: {sample_doc['sections'][0].get('title', 'N/A')}\")\n",
    "        print(f\"First section content length: {len(sample_doc['sections'][0].get('content', ''))}\")\n",
    "else:\n",
    "    print(\"No documents loaded!\")\n",
    "\n",
    "print(f\"\\nAfter processing:\")\n",
    "print(f\"Total text chunks: {len(text_chunks)}\")\n",
    "print(f\"Total section abstracts: {len(section_abstracts)}\")\n",
    "print(f\"Total table sentences: {len(table_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on cuda\n",
      "\n",
      "Generating embeddings...\n",
      "1. Section abstracts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52b7fb60d5f47e2b40e9791133f9eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Text chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1742b7eb4bc401fa8d69c8195cc6fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Table sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50dc117775749f195290da26ea1a71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "# Using a strong open-source model for financial text\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "print(f\"Loading embedding model: {embedding_model_name}\")\n",
    "\n",
    "embedding_generator = EmbeddingGenerator(\n",
    "    model_name=embedding_model_name,\n",
    "    batch_size=32,\n",
    "    device='cuda'  # Use 'cpu' if GPU not available\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "\n",
    "# Generate embeddings for section abstracts (Stage A retrieval)\n",
    "print(\"1. Section abstracts...\")\n",
    "section_embeddings = embedding_generator.encode(\n",
    "    section_abstracts,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Generate embeddings for text chunks (Stage B retrieval)\n",
    "print(\"2. Text chunks...\")\n",
    "text_embeddings = embedding_generator.encode(\n",
    "    text_chunks,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Generate embeddings for table sentences (Stage B retrieval)\n",
    "print(\"3. Table sentences...\")\n",
    "table_embeddings = embedding_generator.encode(\n",
    "    table_sentences,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nEmbedding dimensions: {text_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build FAISS Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS indices...\n",
      "Section index: 14810 vectors\n",
      "Text chunk index: 17706 vectors\n",
      "Table sentence index: 15838 vectors\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index for section abstracts\n",
    "print(\"Building FAISS indices...\")\n",
    "\n",
    "dimension = text_embeddings.shape[1] if len(text_embeddings.shape) > 1 else 768  # Default dimension\n",
    "\n",
    "# Section abstract index (Stage A)\n",
    "if len(section_embeddings.shape) > 1 and section_embeddings.shape[0] > 0:\n",
    "    section_index = faiss.IndexFlatL2(dimension)\n",
    "    section_index.add(section_embeddings.astype('float32'))\n",
    "    print(f\"Section index: {section_index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"Warning: No section embeddings to index\")\n",
    "    section_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Text chunk index (Stage B)\n",
    "if len(text_embeddings.shape) > 1 and text_embeddings.shape[0] > 0:\n",
    "    text_index = faiss.IndexFlatL2(dimension)\n",
    "    text_index.add(text_embeddings.astype('float32'))\n",
    "    print(f\"Text chunk index: {text_index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"Warning: No text embeddings to index\")\n",
    "    text_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Table sentence index (Stage B)\n",
    "if len(table_embeddings.shape) > 1 and table_embeddings.shape[0] > 0:\n",
    "    table_index = faiss.IndexFlatL2(dimension)\n",
    "    table_index.add(table_embeddings.astype('float32'))\n",
    "    print(f\"Table sentence index: {table_index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"Warning: No table embeddings to index (no table sentences extracted)\")\n",
    "    table_index = faiss.IndexFlatL2(dimension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build BM25 Keyword Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BM25 indices...\n",
      "Text BM25 index: 17706 documents\n",
      "Table BM25 index: 15838 documents\n"
     ]
    }
   ],
   "source": [
    "# Tokenize for BM25\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "print(\"Building BM25 indices...\")\n",
    "\n",
    "# BM25 for text chunks\n",
    "if len(text_chunks) > 0:\n",
    "    text_tokens = [simple_tokenize(chunk) for chunk in text_chunks]\n",
    "    text_bm25 = BM25Okapi(text_tokens)\n",
    "    print(f\"Text BM25 index: {len(text_tokens)} documents\")\n",
    "else:\n",
    "    print(\"Warning: No text chunks to index for BM25\")\n",
    "    text_bm25 = None\n",
    "\n",
    "# BM25 for table sentences\n",
    "if len(table_sentences) > 0:\n",
    "    table_tokens = [simple_tokenize(sent) for sent in table_sentences]\n",
    "    table_bm25 = BM25Okapi(table_tokens)\n",
    "    print(f\"Table BM25 index: {len(table_tokens)} documents\")\n",
    "else:\n",
    "    print(\"Warning: No table sentences to index for BM25\")\n",
    "    table_bm25 = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Indices and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving indices...\n",
      "\n",
      "=== Indexing Complete ===\n",
      "Section abstracts indexed: 14810\n",
      "Text chunks indexed: 17706\n",
      "Table sentences indexed: 15838\n",
      "\n",
      "Indices saved to: c:\\Users\\anand\\Desktop\\SEM 3\\CS 582\\Proj\\indices\n"
     ]
    }
   ],
   "source": [
    "# Create indices directory\n",
    "INDICES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving indices...\")\n",
    "\n",
    "# Save FAISS indices\n",
    "faiss.write_index(section_index, str(INDICES_DIR / \"section_index.faiss\"))\n",
    "faiss.write_index(text_index, str(INDICES_DIR / \"text_index.faiss\"))\n",
    "faiss.write_index(table_index, str(INDICES_DIR / \"table_index.faiss\"))\n",
    "\n",
    "# Save content and metadata\n",
    "with open(INDICES_DIR / \"section_data.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'abstracts': section_abstracts,\n",
    "        'metadata': section_abstract_metadata\n",
    "    }, f)\n",
    "\n",
    "with open(INDICES_DIR / \"text_data.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'chunks': text_chunks,\n",
    "        'metadata': chunk_metadata,\n",
    "        'bm25': text_bm25\n",
    "    }, f)\n",
    "\n",
    "with open(INDICES_DIR / \"table_data.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'sentences': table_sentences,\n",
    "        'metadata': table_metadata,\n",
    "        'bm25': table_bm25\n",
    "    }, f)\n",
    "\n",
    "# Save index configuration\n",
    "index_config = {\n",
    "    'embedding_model': embedding_model_name,\n",
    "    'embedding_dimension': dimension,\n",
    "    'num_sections': len(section_abstracts),\n",
    "    'num_text_chunks': len(text_chunks),\n",
    "    'num_table_sentences': len(table_sentences),\n",
    "    'chunk_size': 512,\n",
    "    'chunk_overlap': 50\n",
    "}\n",
    "\n",
    "with open(INDICES_DIR / \"index_config.json\", 'w') as f:\n",
    "    json.dump(index_config, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Indexing Complete ===\")\n",
    "print(f\"Section abstracts indexed: {len(section_abstracts)}\")\n",
    "print(f\"Text chunks indexed: {len(text_chunks)}\")\n",
    "print(f\"Table sentences indexed: {len(table_sentences)}\")\n",
    "print(f\"\\nIndices saved to: {INDICES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Retrieval ===\n",
      "\n",
      "Query: 'R&D expense in 2024'\n",
      "  Top sections:\n",
      "    1. STX 2023 - Section 2...\n",
      "    2. HPE 2023 - Section 10...\n",
      "    3. STE 2023 - Section 10...\n",
      "  Top table sentences:\n",
      "    1. 2024:  $ 3,598    4.1 %...\n",
      "    2. March 1 to March 31, 2024:   —   $  —     —   $  400.0 ...\n",
      "    3. Research and development expense:   109,181    87,581    219,112    82,310    10...\n",
      "  Top text chunks:\n",
      "    1. [SWKS 2023 - Section 10]\n",
      "       new or emerging technologies and changes in customer requirements, or may be able to devote greater ...\n",
      "    2. [NVDA 2023 - Section 2]\n",
      "       2023-01-29 0001045810 us-gaap:CostOfSalesMember 2022-01-31 2023-01-29 0001045810 us-gaap:CostOfSales...\n",
      "    3. [BALL 2024 - Section 2]\n",
      "       2022-12-31 0000009389 us-gaap:NoncontrollingInterestMember 2022-12-31 0000009389 us-gaap:RetainedEar...\n",
      "\n",
      "Query: 'long-term debt to equity ratio'\n",
      "  Top sections:\n",
      "    1. CRM 2024 - Section 3...\n",
      "    2. APA 2022 - Section 1...\n",
      "    3. CRM 2023 - Section 3...\n",
      "  Top table sentences:\n",
      "    1. Emerging market debt(2):  21.1    20.8    1.5     437.1    394.9    10.7  ...\n",
      "    2. Emerging market debt(2):  20.8    20.4    2.0     219.0    541.0    (59.5 ) ...\n",
      "    3. Emerging market debt(2):  29.0    26.8    220  ...\n",
      "  Top text chunks:\n",
      "    1. [APA 2024 - Section 1]\n",
      "       apa-20231231 false 2023 FY 0001841666 33.33 http://fasb.org/us-gaap/2023#DeferredCostsAndOtherAssets...\n",
      "    2. [APA 2023 - Section 1]\n",
      "       apa-20221231 false 2022 FY 0001841666 33.33 66.67 http://fasb.org/us-gaap/2022#OtherAssetsCurrent ht...\n",
      "    3. [CRM 2024 - Section 3]\n",
      "       uritiesMember 2023-01-31 0001108524 us-gaap:EquitySecuritiesMember us-gaap:ChangeDuringPeriodFairVal...\n",
      "\n",
      "Query: 'operating segment revenue growth'\n",
      "  Top sections:\n",
      "    1. FAST 2022 - Section 7...\n",
      "    2. EMN 2022 - Section 1...\n",
      "    3. MOH 2024 - Section 8...\n",
      "  Top table sentences:\n",
      "    1. Operating income:   21 %   24 %   25 %...\n",
      "    2. Operating income:   24 %   25 %   25 %...\n",
      "    3. Total revenue:   2,097.1    1,933.3    8 %   12 %...\n",
      "  Top text chunks:\n",
      "    1. [FAST 2022 - Section 7]\n",
      "       e to supplement our service through our other channels. The following table shows our consolidated n...\n",
      "    2. [FAST 2024 - Section 7]\n",
      "       branch location (2) $ 207.0 199.5 163.6 145.2 140.5 131.1 116.0 104.0 104.0 101.0 Onsite locations 1...\n",
      "    3. [FAST 2023 - Section 9]\n",
      "       — 2 5 7 12 Closed/Converted Branches (6) (115) (5) (1) (121) — — (1) (1) (122) Ending Branches 1,369...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval with sample queries\n",
    "test_queries = [\n",
    "    \"R&D expense in 2024\",\n",
    "    \"long-term debt to equity ratio\",\n",
    "    \"operating segment revenue growth\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Retrieval ===\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = embedding_generator.encode([query])[0]\n",
    "    \n",
    "    # Search section index (Stage A)\n",
    "    distances, indices = section_index.search(\n",
    "        query_embedding.reshape(1, -1).astype('float32'), \n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"  Top sections:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        meta = section_abstract_metadata[idx]\n",
    "        print(f\"    {i+1}. {meta['ticker']} {meta['fiscal_year']} - {meta['section_title'][:60]}...\")\n",
    "    \n",
    "    # Search table index (Stage B) - only if table sentences exist\n",
    "    if len(table_sentences) > 0:\n",
    "        distances, indices = table_index.search(\n",
    "            query_embedding.reshape(1, -1).astype('float32'), \n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        print(\"  Top table sentences:\")\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            print(f\"    {i+1}. {table_sentences[idx][:80]}...\")\n",
    "    else:\n",
    "        print(\"  Top table sentences:\")\n",
    "        print(\"    (No table sentences available - table parsing not implemented)\")\n",
    "    \n",
    "    # Search text chunks (Stage B)\n",
    "    distances, indices = text_index.search(\n",
    "        query_embedding.reshape(1, -1).astype('float32'), \n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"  Top text chunks:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        meta = chunk_metadata[idx]\n",
    "        chunk_preview = text_chunks[idx][:100].replace('\\n', ' ')\n",
    "        print(f\"    {i+1}. [{meta['ticker']} {meta['fiscal_year']} - {meta['section_title']}]\")\n",
    "        print(f\"       {chunk_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **04_retrieval.ipynb** to implement the full hierarchical retrieval pipeline with hybrid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs582",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
