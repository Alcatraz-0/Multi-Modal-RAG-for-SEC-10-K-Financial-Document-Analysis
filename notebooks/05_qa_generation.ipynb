{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - QA Generation: Answer with Citations and Math Verification\n",
    "\n",
    "This notebook implements the answer generation pipeline with LLM reader, math verification, and precise citations.\n",
    "\n",
    "**Objectives:**\n",
    "- Generate answers using LLM (API or local model)\n",
    "- Perform deterministic math verification for numeric answers\n",
    "- Generate precise citations (section + table cell)\n",
    "- Handle conflicting evidence\n",
    "- Ensure faithfulness to retrieved sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Fix OpenMP conflict\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import faiss\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from utils.config import INDICES_DIR, MODEL_DIR\n",
    "from retrieval.hierarchical_retriever import HierarchicalRetriever\n",
    "from retrieval.query_router import QueryRouter\n",
    "from qa.answer_generator import AnswerGenerator\n",
    "from qa.math_verifier import MathVerifier\n",
    "from qa.citation_builder import CitationBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval system loaded\n"
     ]
    }
   ],
   "source": [
    "# Load indices and retrieval components from previous notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load indices\n",
    "section_index = faiss.read_index(str(INDICES_DIR / \"section_index.faiss\"))\n",
    "text_index = faiss.read_index(str(INDICES_DIR / \"text_index.faiss\"))\n",
    "table_index = faiss.read_index(str(INDICES_DIR / \"table_index.faiss\"))\n",
    "\n",
    "with open(INDICES_DIR / \"section_data.pkl\", 'rb') as f:\n",
    "    section_data = pickle.load(f)\n",
    "with open(INDICES_DIR / \"text_data.pkl\", 'rb') as f:\n",
    "    text_data = pickle.load(f)\n",
    "with open(INDICES_DIR / \"table_data.pkl\", 'rb') as f:\n",
    "    table_data = pickle.load(f)\n",
    "with open(INDICES_DIR / \"index_config.json\", 'r') as f:\n",
    "    index_config = json.load(f)\n",
    "\n",
    "# Initialize components\n",
    "embedding_model = SentenceTransformer(index_config['embedding_model'])\n",
    "query_router = QueryRouter()\n",
    "hierarchical_retriever = HierarchicalRetriever(\n",
    "    section_index, text_index, table_index,\n",
    "    section_data, text_data, table_data,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "print(\"Retrieval system loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Answer Generation Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing answer generator with Ollama...\n",
      "Model: llama3.2:1b\n",
      "Make sure Ollama is running: ollama serve\n",
      "\n",
      "✓ Ollama client initialized (model: llama3.2:1b)\n",
      "✓ Math verifier initialized\n",
      "✓ Citation builder initialized\n",
      "\n",
      "✓ All QA components ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize QA components\n",
    "\n",
    "# Answer Generator with Ollama\n",
    "USE_OLLAMA = True  # Using Ollama\n",
    "OLLAMA_MODEL = \"llama3.2:1b\"  # You can change this to: mistral, phi3, llama3, etc.\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"  # Default Ollama URL\n",
    "\n",
    "USE_OPENAI = False  # Set to True if you want to use OpenAI instead\n",
    "OPENAI_API_KEY = \"your-api-key-here\"  # Only needed if USE_OPENAI = True\n",
    "\n",
    "print(\"Initializing answer generator with Ollama...\")\n",
    "print(f\"Model: {OLLAMA_MODEL}\")\n",
    "print(f\"Make sure Ollama is running: ollama serve\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    answer_generator = AnswerGenerator(\n",
    "        use_ollama=USE_OLLAMA,\n",
    "        ollama_model=OLLAMA_MODEL,\n",
    "        ollama_base_url=OLLAMA_BASE_URL,\n",
    "        use_openai=USE_OPENAI,\n",
    "        openai_api_key=OPENAI_API_KEY if USE_OPENAI else None,\n",
    "        max_length=512,\n",
    "        temperature=0.1  # Low temperature for factual answers\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing answer generator: {e}\")\n",
    "    print(\"\\nIf Ollama is not running, start it with: ollama serve\")\n",
    "    print(f\"Then pull the model: ollama pull {OLLAMA_MODEL}\")\n",
    "    raise\n",
    "\n",
    "# Math Verifier\n",
    "math_verifier = MathVerifier(tolerance=0.01)  # 1% tolerance\n",
    "print(\"✓ Math verifier initialized\")\n",
    "\n",
    "# Citation Builder\n",
    "citation_builder = CitationBuilder()\n",
    "print(\"✓ Citation builder initialized\")\n",
    "\n",
    "print(\"\\n✓ All QA components ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Answer Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_citations(query: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete QA pipeline: retrieve → generate → verify → cite\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # 1. Route query\n",
    "    route_info = query_router.route(query)\n",
    "    if verbose:\n",
    "        print(f\"Query type: {route_info['query_type']}\")\n",
    "        print(f\"Table-centric: {route_info['is_table_centric']}\")\n",
    "        print(f\"Requires math: {route_info['requires_math']}\\n\")\n",
    "    \n",
    "    # 2. Retrieve evidence\n",
    "    retrieval_results = hierarchical_retriever.retrieve(\n",
    "        query=query,\n",
    "        route_info=route_info,\n",
    "        top_k_sections=5,\n",
    "        top_k_content=10,\n",
    "        use_hybrid=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Retrieved {len(retrieval_results['content'])} evidence pieces\\n\")\n",
    "    \n",
    "    # 3. Generate answer\n",
    "    answer_result = answer_generator.generate(\n",
    "        query=query,\n",
    "        evidence=retrieval_results['content'],\n",
    "        route_info=route_info\n",
    "    )\n",
    "    \n",
    "    answer_text = answer_result['answer']\n",
    "    if verbose:\n",
    "        print(f\"Generated answer: {answer_text}\\n\")\n",
    "    \n",
    "    # 4. Math verification (if needed)\n",
    "    verification_result = None\n",
    "    if route_info['requires_math']:\n",
    "        verification_result = math_verifier.verify(\n",
    "            answer=answer_text,\n",
    "            evidence=retrieval_results['content'],\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Math verification: {verification_result['status']}\")\n",
    "            if verification_result['status'] != 'verified':\n",
    "                print(f\"  Issue: {verification_result['message']}\")\n",
    "            print()\n",
    "    \n",
    "    # 5. Build citations\n",
    "    citations = citation_builder.build_citations(\n",
    "        answer=answer_text,\n",
    "        evidence=retrieval_results['content']\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Citations:\")\n",
    "        for i, citation in enumerate(citations):\n",
    "            print(f\"  [{i+1}] {citation}\")\n",
    "    \n",
    "    # 6. Compile final result\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer_text,\n",
    "        'citations': citations,\n",
    "        'evidence': retrieval_results['content'],\n",
    "        'route_info': route_info,\n",
    "        'verification': verification_result,\n",
    "        'confidence': answer_result.get('confidence', None)\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test QA Pipeline: Numeric Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NUMERIC QUESTION TESTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: Report the YoY change in R&D expense for 2022 to 2024\n",
      "\n",
      "Query type: numeric_table\n",
      "Table-centric: True\n",
      "Requires math: True\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: Based on the evidence provided, the YoY change in R&D expense for 2022 to 2024 is as follows:\n",
      "\n",
      "R&D expense in 2023 was $1.23B (Table T8, Row 2).\n",
      "R&D expense in 2024 was $1.45B (Table T7, Row 1).\n",
      "\n",
      "The YoY change in R&D expense from 2022 to 2024 is a $22M increase ($1.45B - $1.23B).\n",
      "\n",
      "Math verification: verified\n",
      "\n",
      "Citations:\n",
      "  [1] CMG 2023 10-K, Unknown, Table T8, Row 2\n",
      "  [2] CMG 2024 10-K, Unknown, Table T8, Row 2\n",
      "  [3] TEL 2024 10-K, Unknown, Table T6, Row 1\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: What is the ratio of long-term debt to equity in 2023?\n",
      "\n",
      "Query type: numeric_table\n",
      "Table-centric: True\n",
      "Requires math: True\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: Based on the provided evidence, we can calculate the ratio of long-term debt to equity in 2023 as follows:\n",
      "\n",
      "[TABLE] HCA 2024 - Table T6, Row 1\n",
      ":  2022   Ratio   2021   Ratio   2020   Ratio  None None None None None\n",
      "\n",
      "To find the ratio of long-term debt to equity, we need to calculate the total long-term debt and total equity.\n",
      "\n",
      "Total long-term debt in 2023:\n",
      "[TABLE] HCA 2024 - Table T6, Row 2\n",
      ":  2023   Total Long-Term Debt   $1.42B\n",
      "\n",
      "Total equity in 2023:\n",
      "[TABLE] MTB 2023 - Table T8, Row 0\n",
      ": II. Investments in debt securities   None\n",
      "\n",
      "Since there is no information on long-term debt issuance or equity issuance in 2022, we cannot calculate the ratio.\n",
      "\n",
      "Therefore, the ratio of long-term debt to equity in 2023 is not available based on the provided evidence.\n",
      "\n",
      "Math verification: failed\n",
      "  Issue: Calculations do not match evidence\n",
      "\n",
      "Citations:\n",
      "  [1] MTB 2023 10-K, Unknown, Table T8, Row 0\n",
      "  [2] D 2024 10-K, Unknown, Table T9, Row 21\n",
      "  [3] HCA 2024 10-K, Unknown, Table T6, Row 1\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: Which operating segment contributed most to 2024 revenue growth, and by how much?\n",
      "\n",
      "Query type: numeric_table\n",
      "Table-centric: True\n",
      "Requires math: True\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: Based on the evidence provided, Gas Distribution contributed the most to Dominion Energy Virginia's 2024 revenue growth, with a growth rate of 14.1% compared to the previous year.\n",
      "\n",
      "According to Table T10, Row 26, Gas Distribution's contracted assets grew by $2.5 million in 2024 (calculated as $3.8 million - $1.3 million = $2.5 million), resulting in a revenue increase of $7.6 million (calculated as $2.5 million x $3,000 per million = $7.6 million).\n",
      "\n",
      "Similarly, Contracted Energy's contracted assets grew by 10.4% in 2024 ($1.8 million - $1.6 million = $0.2 million) and its revenue increased by 12.9% ($1.5 million - $1.3 million = $0.2 million), resulting in a combined growth rate of 23.3%.\n",
      "\n",
      "Therefore, Gas Distribution contributed the most to Dominion Energy Virginia's 2024 revenue growth, with a growth rate of 14.1%.\n",
      "\n",
      "Math verification: failed\n",
      "  Issue: Calculations do not match evidence\n",
      "\n",
      "Citations:\n",
      "  [1] D 2022 10-K, Unknown, Table T10, Row 2\n",
      "  [2] D 2022 10-K, Unknown, Table T10, Row 26\n",
      "  [3] D 2022 10-K, Unknown, Table T9, Row 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with numeric questions requiring math\n",
    "numeric_queries = [\n",
    "    \"Report the YoY change in R&D expense for 2022 to 2024\",\n",
    "    \"What is the ratio of long-term debt to equity in 2023?\",\n",
    "    \"Which operating segment contributed most to 2024 revenue growth, and by how much?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NUMERIC QUESTION TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_results = []\n",
    "for query in numeric_queries:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    result = generate_answer_with_citations(query, verbose=True)\n",
    "    numeric_results.append(result)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test QA Pipeline: Narrative Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NARRATIVE QUESTION TESTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: What are the main business segments?\n",
      "\n",
      "Query type: narrative\n",
      "Table-centric: False\n",
      "Requires math: False\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: Based on the provided evidence, the main business segments of TJX Companies, Inc. are:\n",
      "\n",
      "1. Marmaxx and HomeGoods, both in the U.S.\n",
      "2. TJX Canada and TJX International, including Europe and Australia\n",
      "3. Sierra, acquired in 2012 and rebranded from Sierra Trading Post in 2018\n",
      "\n",
      "These three segments operate under the following sub-segments:\n",
      "\n",
      "* Marmaxx:\n",
      "\t+ TJ Maxx and Marshalls chains in the United States (collectively the largest off-price retailer in the United States with a total of 2,482 stores)\n",
      "* HomeGoods:\n",
      "\t+ HomeGoods and Homesense chains\n",
      "* Sierra:\n",
      "\t+ Sierra Trading Post (acquired in 2012)\n",
      "\n",
      "Citations:\n",
      "  [1] TJX 2023 10-K, Section 10\n",
      "  [2] TJX 2024 10-K, Section 10\n",
      "  [3] PNR 2022 10-K, Section 8\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: Explain the primary risk factors mentioned in the filing\n",
      "\n",
      "Query type: narrative\n",
      "Table-centric: False\n",
      "Requires math: False\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: The primary risk factors mentioned in the filing are:\n",
      "\n",
      "1. Risk Factors related to the COVID-19 pandemic, including its impact on our business, financial condition and liquidity.\n",
      "2. The sensitivity of our business to the number of flight hours that our customers' planes spend aloft and our customers' profitability.\n",
      "\n",
      "These risks are described as \"adversely affecting\" and \"could continue to adversely affect\" our financial results, operations, and outlook for an extended period of time.\n",
      "\n",
      "Citations:\n",
      "  [1] CMG 2022 10-K, Section 9\n",
      "  [2] TDG 2023 10-K, Section 9\n",
      "  [3] CTAS 2022 10-K, Section 9\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: What is the company's business strategy?\n",
      "\n",
      "Query type: narrative\n",
      "Table-centric: False\n",
      "Requires math: False\n",
      "\n",
      "Retrieved 10 evidence pieces\n",
      "\n",
      "Generated answer: The company's business strategy can be inferred from various sections of the report. According to Section 9 of the 2023 Form 10-K, Grainger's purpose is \"to keep the world working,\" which allows customers to focus on their core businesses and do what they do best.\n",
      "\n",
      "Section 8 of the 2022 Form 10-K also mentions that Grainger's two reportable segments are High-Touch Solutions N.A. and Endless Assortment, aligning with Grainger's go-to-market strategies and bifurcated business models of high-touch solutions and endless assortment.\n",
      "\n",
      "Additionally, Section 9 of the 2023 Form 10-K describes Grainger's strategic framework, \"The Grainger Edge,\" which defines the Company by asserting why it exists, how it serves customers, and how team members work together to achieve its objectives. This framework also outlines a set of principles that define the behaviors expected from Grainger's team members in working with each other and the Company's customers, suppliers, and communities.\n",
      "\n",
      "Furthermore, Section 8 of the 2022 Form 10-K mentions that Grainger's business models allow it to leverage its scale and advantaged supply chain to meet changing customer needs. This suggests that Grainger is focused on providing value-added solutions to its customers.\n",
      "\n",
      "Overall, the company's business strategy appears to be centered around providing high-quality products and services to its customers, while also leveraging its scale and advantaged supply chain to drive growth and profitability.\n",
      "\n",
      "Citations:\n",
      "  [1] GWW 2023 10-K, Section 9\n",
      "  [2] GWW 2022 10-K, Section 8\n",
      "  [3] GWW 2024 10-K, Section 8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with narrative/explanation questions\n",
    "narrative_queries = [\n",
    "    \"What are the main business segments?\",\n",
    "    \"Explain the primary risk factors mentioned in the filing\",\n",
    "    \"What is the company's business strategy?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NARRATIVE QUESTION TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "narrative_results = []\n",
    "for query in narrative_queries:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    result = generate_answer_with_citations(query, verbose=True)\n",
    "    narrative_results.append(result)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Math Verification Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Calculate the YoY percentage change in revenue from 2023 to 2024\n",
      "\n",
      "=== Revenue Evidence ===\n",
      "1. : • the percentage of our total revenue from various end markets,\n",
      "   Source: Table T8, Row 21\n",
      "\n",
      "2. :   Year EndedDecember 31,  Percentage change in revenue as reported  Impact ofchanges inforeigncurrency (a)  Percentage change in revenue on a constant currency basis (a) None None None\n",
      "   Source: Table T8, Row 0\n",
      "\n",
      "3. :  Year Ended December 31,   Percentage change in revenue   Impact ofchanges inforeigncurrency (a)   Percentage change in revenue on a constant currency basis (a)  None None None None None None None None\n",
      "   Source: Table T8, Row 0\n",
      "\n",
      "\n",
      "=== Extracted Numbers ===\n",
      "[31.0, 31.0, 31.0, 2023.0, 2022.0, 2021.0]\n",
      "\n",
      "=== Verification Result ===\n",
      "Status: failed\n",
      "Message: Calculations do not match evidence\n"
     ]
    }
   ],
   "source": [
    "# Detailed math verification example\n",
    "query = \"Calculate the YoY percentage change in revenue from 2023 to 2024\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Retrieve evidence\n",
    "route_info = query_router.route(query)\n",
    "evidence = hierarchical_retriever.retrieve(\n",
    "    query=query,\n",
    "    route_info=route_info,\n",
    "    top_k_content=10\n",
    ")['content']\n",
    "\n",
    "# Find table rows with revenue data\n",
    "revenue_evidence = [\n",
    "    e for e in evidence \n",
    "    if 'revenue' in e['content'].lower() and e['metadata']['content_type'] == 'table'\n",
    "]\n",
    "\n",
    "print(\"=== Revenue Evidence ===\")\n",
    "for i, ev in enumerate(revenue_evidence[:3]):\n",
    "    print(f\"{i+1}. {ev['content']}\")\n",
    "    print(f\"   Source: Table {ev['metadata']['table_id']}, Row {ev['metadata']['row_idx']}\\n\")\n",
    "\n",
    "# Extract numbers and perform calculation\n",
    "numbers = math_verifier.extract_numbers(revenue_evidence)\n",
    "print(\"\\n=== Extracted Numbers ===\")\n",
    "print(numbers)\n",
    "\n",
    "# Verify calculation\n",
    "answer = \"Revenue increased 12.5% YoY from $100M in 2023 to $112.5M in 2024\"\n",
    "verification = math_verifier.verify(\n",
    "    answer=answer,\n",
    "    evidence=revenue_evidence,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(\"\\n=== Verification Result ===\")\n",
    "print(f\"Status: {verification['status']}\")\n",
    "print(f\"Message: {verification['message']}\")\n",
    "if 'calculation_details' in verification:\n",
    "    print(f\"Details: {verification['calculation_details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Citation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Citation Quality Analysis ===\n",
      "Total answers: 6\n",
      "Avg citations per answer: 3.00\n",
      "Citation types: {'text': 9, 'table': 9}\n",
      "Answers with table citations: 3\n",
      "Answers with cell-level citations: 0\n"
     ]
    }
   ],
   "source": [
    "# Analyze citation quality and precision\n",
    "def analyze_citations(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze citation quality across multiple QA results\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_answers': len(results),\n",
    "        'avg_citations_per_answer': [],\n",
    "        'citation_types': {'text': 0, 'table': 0},\n",
    "        'answers_with_table_citations': 0,\n",
    "        'answers_with_cell_level_citations': 0\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        citations = result['citations']\n",
    "        stats['avg_citations_per_answer'].append(len(citations))\n",
    "        \n",
    "        has_table = False\n",
    "        has_cell_level = False\n",
    "        \n",
    "        for citation in citations:\n",
    "            if 'table' in citation.lower():\n",
    "                stats['citation_types']['table'] += 1\n",
    "                has_table = True\n",
    "                \n",
    "                if 'row' in citation.lower() and 'column' in citation.lower():\n",
    "                    has_cell_level = True\n",
    "            else:\n",
    "                stats['citation_types']['text'] += 1\n",
    "        \n",
    "        if has_table:\n",
    "            stats['answers_with_table_citations'] += 1\n",
    "        if has_cell_level:\n",
    "            stats['answers_with_cell_level_citations'] += 1\n",
    "    \n",
    "    stats['avg_citations_per_answer'] = np.mean(stats['avg_citations_per_answer'])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze all results\n",
    "all_results = numeric_results + narrative_results\n",
    "citation_stats = analyze_citations(all_results)\n",
    "\n",
    "print(\"=== Citation Quality Analysis ===\")\n",
    "print(f\"Total answers: {citation_stats['total_answers']}\")\n",
    "print(f\"Avg citations per answer: {citation_stats['avg_citations_per_answer']:.2f}\")\n",
    "print(f\"Citation types: {citation_stats['citation_types']}\")\n",
    "print(f\"Answers with table citations: {citation_stats['answers_with_table_citations']}\")\n",
    "print(f\"Answers with cell-level citations: {citation_stats['answers_with_cell_level_citations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Formatted Output with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUESTION: Report the YoY change in R&D expense for 2022 to 2024\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "Based on the evidence provided, the YoY change in R&D expense for 2022 to 2024 is as follows:\n",
      "\n",
      "R&D expense in 2023 was $1.23B (Table T8, Row 2).\n",
      "R&D expense in 2024 was $1.45B (Table T7, Row 1).\n",
      "\n",
      "The YoY change in R&D expense from 2022 to 2024 is a $22M increase ($1.45B - $1.23B).\n",
      "\n",
      "VERIFICATION:\n",
      "  Status: verified\n",
      "\n",
      "SOURCES:\n",
      "  [1] CMG 2023 10-K, Unknown, Table T8, Row 2\n",
      "  [2] CMG 2024 10-K, Unknown, Table T8, Row 2\n",
      "  [3] TEL 2024 10-K, Unknown, Table T6, Row 1\n",
      "\n",
      "EVIDENCE:\n",
      "  1. [TABLE] CMG 2023\n",
      "     : 2022  2021  change None None...\n",
      "  2. [TABLE] CMG 2024\n",
      "     : 2023  2022  change None None...\n",
      "  3. [TABLE] TEL 2024\n",
      "     ​:  2024   2023   2022  ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def format_qa_output(result: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format QA result for presentation\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(f\"QUESTION: {result['query']}\")\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"\\nANSWER:\")\n",
    "    output.append(result['answer'])\n",
    "    \n",
    "    if result['verification']:\n",
    "        output.append(\"\\nVERIFICATION:\")\n",
    "        output.append(f\"  Status: {result['verification']['status']}\")\n",
    "        if result['verification']['status'] != 'verified':\n",
    "            output.append(f\"  Note: {result['verification']['message']}\")\n",
    "    \n",
    "    output.append(\"\\nSOURCES:\")\n",
    "    for i, citation in enumerate(result['citations']):\n",
    "        output.append(f\"  [{i+1}] {citation}\")\n",
    "    \n",
    "    output.append(\"\\nEVIDENCE:\")\n",
    "    for i, ev in enumerate(result['evidence'][:3]):\n",
    "        meta = ev['metadata']\n",
    "        output.append(f\"  {i+1}. [{meta['content_type'].upper()}] {meta['ticker']} {meta['fiscal_year']}\")\n",
    "        output.append(f\"     {ev['content'][:150]}...\")\n",
    "    \n",
    "    output.append(\"=\" * 80)\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Display formatted output for first result\n",
    "if all_results:\n",
    "    print(format_qa_output(all_results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save QA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6 QA results to c:\\Users\\anand\\Desktop\\SEM 3\\CS 582\\Proj\\data\\qa_results\\qa_results.json\n",
      "Saved formatted output to c:\\Users\\anand\\Desktop\\SEM 3\\CS 582\\Proj\\data\\qa_results\\qa_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save QA results for evaluation\n",
    "results_dir = Path.cwd().parent / 'data' / 'qa_results'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "output_file = results_dir / 'qa_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    # Convert to serializable format\n",
    "    serializable_results = []\n",
    "    for result in all_results:\n",
    "        serializable_results.append({\n",
    "            'query': result['query'],\n",
    "            'answer': result['answer'],\n",
    "            'citations': result['citations'],\n",
    "            'route_info': result['route_info'],\n",
    "            'verification': result['verification'],\n",
    "            'num_evidence': len(result['evidence'])\n",
    "        })\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(all_results)} QA results to {output_file}\")\n",
    "\n",
    "# Save formatted text output\n",
    "text_output_file = results_dir / 'qa_results.txt'\n",
    "with open(text_output_file, 'w') as f:\n",
    "    for result in all_results:\n",
    "        f.write(format_qa_output(result))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Saved formatted output to {text_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **06_evaluation.ipynb** to evaluate the system on benchmark datasets (FinQA, DocFinQA, FinDER) and compute metrics."
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "cs582",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
